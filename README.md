# tinyBPE - Trainable tokenizer based on Byte-Pair Encoding similar to GPT-2 and GPT-4

For LLM to understand texts it needs a translator between text and number, that is called ```Tokenizer```. For LLMs Byte Pair Encoding is the most used algorithm to avoid very little character level tokenizers and very huge word level/ n-gram level tokenizers.